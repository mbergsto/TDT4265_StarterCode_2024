{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 1 Report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is an outline for your report to ease the amount of work required to create your report. Jupyter notebook supports markdown, and I recommend you to check out this [cheat sheet](https://github.com/adam-p/markdown-here/wiki/Markdown-Cheatsheet). If you are not familiar with markdown.\n",
    "\n",
    "Before delivery, **remember to convert this file to PDF**. You can do it in two ways:\n",
    "1. Print the webpage (ctrl+P or cmd+P)\n",
    "2. Export with latex. This is somewhat more difficult, but you'll get somehwat of a \"prettier\" PDF. Go to File -> Download as -> PDF via LaTeX. You might have to install nbconvert and pandoc through conda; `conda install nbconvert pandoc`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## task 1a)\n",
    "\n",
    "![Handwritten notes for Task 1a](<images//task_1a.png>)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## task 1b)\n",
    "\n",
    "![Handwritten notes for Task 1b](<images//task_1b.png>)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2a)\n",
    "\n",
    "Mean:  33.55274553571429  \n",
    "Std:  78.87550070784701\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2c)\n",
    "![](task2c_train_loss.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2d)\n",
    "Parameters in the Hidden Layer\n",
    "\n",
    "For the hidden layer, each of the 64 nodes receives input from all 784 input nodes, plus a bias term:\n",
    "\n",
    "Weights: 784 * 64 = 50176\n",
    "\n",
    "Biases: 64\n",
    "\n",
    "The total number of parameters for the hidden layer is 50176 + 64 = 50240\n",
    "\n",
    "Parameters in the Output Layer\n",
    "\n",
    "For the output layer, each of the 10 output nodes receives input from all 64 hidden nodes, plus a bias term:\n",
    "\n",
    "Weights: 65×10=650 (including the bias unit from the hidden layer)\n",
    "\n",
    "Biases: 10\n",
    "\n",
    "The total number of parameters for the output layer is 650+10=660\n",
    "\n",
    "Total Parameters in the Network\n",
    "\n",
    "Adding these together, the total number of parameters in the network is:\n",
    "\n",
    "50240 (hidden layer) + 660 (output layer) = 50900 \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we incorporated the improvements to the model, it lead to enhanced performance in terms of learning speed, convergence, generalization and final accuracy/validation loss. \n",
    "\n",
    "The initialization of input weights using a normal distribution with a standard deviation scaled by the square root of the fan-in helps in initializing weights suitable for the scale of inputs. This lead to faster convergence during training, because the weights start closer to their optimal values. \n",
    "\n",
    "Using the improved sigmoid activation function in the hidden layer allowed for a more complex representation of the data, which reduces overfitting, as viewed in the plots. Applying momentum to the gradient update step can help in avoiding overfitting.\n",
    "\n",
    "With the improvements, we see improvements in final accuracy and in validation loss. The improved initialization function helps the model learn better representation of the data, while the momentum can help in escaping local minima and reaching a more optimal solution. \n",
    "\n",
    "![](task3_model_comparison.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4a & b)\n",
    "\n",
    "When we chose a low number of hidden units, the training worked as normal, where the only difference was the accuracy not being as good as for higher numbers of hidden units. The same was true when we increased the hidden units to a “too large” number. The accuracy became better, and we did not see any other consequences. \n",
    "\n",
    "We expected to see an overfitted model when the number of hidden units was low, and an overfitted model when it was high, but this was not the case here. \n",
    "\n",
    "![](task4ab.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4c)\n",
    "\n",
    "We were unable to find a good solution on this task. The gradient would not calculate correctly, so the tests failed. \n",
    "\n",
    "We were therefore also unable to solve d) and e). \n",
    "\n",
    "Since we failed to implement task 4c), the SoftmaxModel does not work as it did in earlier tasks.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.1 64-bit ('py38': conda)",
   "language": "python",
   "name": "python38164bitpy38condac1f68ca5407a4349b0d7e37676f2fbb3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
