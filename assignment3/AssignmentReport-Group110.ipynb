{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 1 Report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is an outline for your report to ease the amount of work required to create your report. Jupyter notebook supports markdown, and I recommend you to check out this [cheat sheet](https://github.com/adam-p/markdown-here/wiki/Markdown-Cheatsheet). If you are not familiar with markdown.\n",
    "\n",
    "Before delivery, **remember to convert this file to PDF**. You can do it in two ways:\n",
    "1. Print the webpage (ctrl+P or cmd+P)\n",
    "2. Export with latex. This is somewhat more difficult, but you'll get somehwat of a \"prettier\" PDF. Go to File -> Download as -> PDF via LaTeX. You might have to install nbconvert and pandoc through conda; `conda install nbconvert pandoc`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## task 1a)\n",
    "\n",
    "![](Images/Task1a-1.jpg)\n",
    "![](Images/Task1a-2.jpg)\n",
    "![](Images/Task1a-3.jpeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## task 1b)\n",
    "Answer is:\n",
    "(iii) Max Pooling reduces the sensitivity to translational variations in the input by choosing the maximum value of a small region, thereby making the output invariant to small shifts and distortions.\n",
    "\n",
    "## task 1c)\n",
    "\n",
    "![](Images/Task1c.jpeg)\n",
    "\n",
    "\n",
    "## task 1d)\n",
    "\n",
    "![](Images/Task1d.jpeg)\n",
    "\n",
    "## task 1e)\n",
    "\n",
    "![](Images/Task1e.jpeg)\n",
    "## task 1f)\n",
    "\n",
    "![](Images/Task1f.jpeg)\n",
    "## task 1g)\n",
    "\n",
    "![](Images/Task1g-1.jpg)\n",
    "![](Images/Task1g-2.jpg)\n",
    "![](Images/Task1g-3.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2\n",
    "\n",
    "### Task 2a)\n",
    "\n",
    "Epoch: 0, Batches per seconds: 278.71, Global step:    351, Validation Loss: 1.71, Validation Accuracy: 0.375\\\n",
    "Epoch: 0, Batches per seconds: 258.48, Global step:    702, Validation Loss: 1.38, Validation Accuracy: 0.502\\\n",
    "Epoch: 1, Batches per seconds: 254.40, Global step:   1053, Validation Loss: 1.33, Validation Accuracy: 0.503\\\n",
    "Epoch: 1, Batches per seconds: 256.43, Global step:   1404, Validation Loss: 1.24, Validation Accuracy: 0.564\\\n",
    "Epoch: 2, Batches per seconds: 253.93, Global step:   1755, Validation Loss: 1.09, Validation Accuracy: 0.612\\\n",
    "Epoch: 2, Batches per seconds: 254.71, Global step:   2106, Validation Loss: 0.97, Validation Accuracy: 0.660\\\n",
    "Epoch: 3, Batches per seconds: 254.61, Global step:   2457, Validation Loss: 0.90, Validation Accuracy: 0.683\\\n",
    "Epoch: 3, Batches per seconds: 256.22, Global step:   2808, Validation Loss: 0.99, Validation Accuracy: 0.645\\\n",
    "Epoch: 4, Batches per seconds: 257.73, Global step:   3159, Validation Loss: 0.86, Validation Accuracy: 0.701\\\n",
    "Epoch: 4, Batches per seconds: 258.32, Global step:   3510, Validation Loss: 0.83, Validation Accuracy: 0.710\\\n",
    "Epoch: 5, Batches per seconds: 257.61, Global step:   3861, Validation Loss: 0.81, Validation Accuracy: 0.719\\\n",
    "Epoch: 5, Batches per seconds: 258.77, Global step:   4212, Validation Loss: 0.78, Validation Accuracy: 0.733\\\n",
    "Epoch: 6, Batches per seconds: 257.56, Global step:   4563, Validation Loss: 0.79, Validation Accuracy: 0.734\\\n",
    "Epoch: 6, Batches per seconds: 260.10, Global step:   4914, Validation Loss: 0.82, Validation Accuracy: 0.733\\\n",
    "Epoch: 7, Batches per seconds: 260.24, Global step:   5265, Validation Loss: 0.79, Validation Accuracy: 0.736\\\n",
    "Early stop criteria met\\\n",
    "Early stopping.\\\n",
    "\n",
    "![](../plots/task2_plot.png)\n",
    "\n",
    "### Task 2b)\n",
    "Train accuracy: 0.757, Validation accuracy: 0.734, Test accuracy: 0.756\\\n",
    "Train loss: 0.693, Validation loss: 0.761, Test loss: 0.713"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3a)\n",
    "\n",
    "\n",
    "| Layer | Layer Type         | Number of Hidden Units / Number of Filters | Activation Function |\n",
    "|-------|--------------------|--------------------------------------------|---------------------|\n",
    "| 1     | Conv2D             | 128                                        | LeakyReLU           |\n",
    "| 1     | BatchNorm2d (128)  | -                                          | -                   |\n",
    "| 1     | Dropout2d (p=0.05) | -                                          | -                   |\n",
    "| 1     | MaxPool2D          | -                                          | -                   |\n",
    "| 2     | Conv2D             | 256                                        | LeakyReLU           |\n",
    "| 2     | BatchNorm2d (256)  | -                                          | -                   |\n",
    "| 2     | Dropout2d (p=0.05) | -                                          | -                   |\n",
    "| 2     | MaxPool2D          | -                                          | -                   |\n",
    "| 3     | Conv2D             | 512                                        | LeakyReLU           |\n",
    "| 3     | BatchNorm2d (512)  | -                                          | -                   |\n",
    "| 3     | Dropout2d (p=0.05) | -                                          | -                   |\n",
    "| 3     | MaxPool2D          | -                                          | -                   |\n",
    "| 4     | Conv2D             | 1024                                        | LeakyReLU           |\n",
    "| 4     | BatchNorm2d (1024)  | -                                          | -                   |\n",
    "| 4     | Dropout2d (p=0.05) | -                                          | -                   |\n",
    "| 4     | MaxPool2D          | -                                          | -                   |\n",
    "| -     | Flatten            | -                                          | -                   |\n",
    "| 5     | Fully-Connected    | 512                                        | LeakyReLU           |\n",
    "| 5     | BatchNorm1d (512)  | -                                          | -                   |\n",
    "| 5     | Dropout (p=0.05)   | -                                          | -                   |\n",
    "| 6     | Fully-Connected    | 64                                         | LeakyReLU           |\n",
    "| 6     | BatchNorm1d (64)   | -                                          | -                   |\n",
    "| 6     | Dropout (p=0.05)   | -                                          | -                   |\n",
    "| 7     | Fully-Connected    | 10 (num_classes)                           |\n",
    "Optimizer: Stochastic Gradient Descent (SGD)\\\n",
    "Learning rate: 5e-2\\\n",
    "Batch size: 64\\\n",
    "Regularization methods:\n",
    "- Dropout: Applied with a probability of 0.05\n",
    "- Batch Normalization: Applied after each convolutional and fully-connected layer.\n",
    "- Data Augmentation: Applied on training data in dataloaders.py, if \"augment\" is True.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### Task 3b)\n",
    "| Metric             | Train  | Validation | Test  |\n",
    "|--------------------|--------|------------|-------|\n",
    "| Accuracy           | 0.835  | 0.774      | 0.811 |\n",
    "| Loss               | 0.475  | 0.644      | 0.562 |\n",
    "\n",
    "\n",
    "![](../plots/task3_plot.png)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### Task 3c)\n",
    "\n",
    "\n",
    "#### What improved the test accuracy:\n",
    "\n",
    "\n",
    "1. **Added Batch Normalization**: This enhances the stability of the network by normalizing the inputs to any activation layer. It helps in faster convergence and also has a regularization effect, reducing the chance of overfitting. By ensuring that the distribution of the inputs to layers deep in the network stays stable, it helps the network learn more effectivel, and thereby increasing the test accuracy.\n",
    "\n",
    "\n",
    "2. **Decreased Kernel Size from 5x5 to 3x3**: Smaller kernels require fewer parameters, reducing the computational cost and the risk of overfitting. Furthermore, by using a 3x3 kernel instead of a 5x5, the network can retain more fine-grained details in the feature map. This improved the test accuracy.\n",
    "\n",
    "\n",
    "3. **Changed ReLU to LeakyReLU**: While ReLU is an effective activation function for introducing non-linearity into the network, it has a drawback known as the \"dying ReLU\" problem, where neurons can sometimes permanently output zeros due to negative input values. LeakyReLU addresses this by allowing a small, positive gradient when the unit is not active and not strictly zero. This change can lead to better learning and prevent neurons from becoming inactive.\n",
    "\n",
    "\n",
    "4. **Increased number of filters**: Increasing the number of output channels in a convolutional layer means that the layer can learn a larger variety of features from the input data. This can significantly enhance the network's ability to represent complex functions and distinguish between more advanced patterns in the data.\n",
    "\n",
    "\n",
    "5. **Added One More Layer**: Adding more layers to a network increases its depth, potentially improving its ability to learn more complex features. However, this comes with the risk of overfitting and increased computational cost. Deeper networks can represent more complex functions but require more data and regularization techniques to train effectively.\n",
    "\n",
    "\n",
    "6. **Added Data Augmentation**: This technique involves artificially expanding the training dataset by applying random transformations (such as rotation, scaling, and cropping). This can significantly improve the model's generalization capability by forcing it to learn invariant to these transformations, reducing the risk of overfitting on the training data. It's particularly beneficial when the available dataset is limited in size. It is also necessary considering we added an extra layer.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### What decreased test accuracy\n",
    "\n",
    "\n",
    "1. **Decreased number of filters**: When we tries decreasing the output channels in the convulational layers, the layer became too simple, and not able to represent the complex functions good enough. This strongly decreased the test accuracy.\n",
    "\n",
    "\n",
    "2. **Decreased number of layers**: When we added a lauer to a network it decreased the depth, which worsened its ability to learn complex features. The risk of overfitting became smaller, and the computational cost was lower. The test accuracy got a lot worse from this.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### Task 3d)\n",
    "Below is the plots of loss and accuracy from before and after the improvements. Note that the plots of validation accuracy is seemingly lower in the after version. This is because of the data augmentation implementation. This increased the training dataset-size and made it more general than the previous training. It made it harder to validate during the training, but improved the overall test accuracy when the model was done training.\n",
    "\n",
    "\n",
    "**Before improvements**:\n",
    "![](Images/Task3d-before.png)\n",
    "\n",
    "\n",
    "**After improvements**:\n",
    "![](../plots//Task3_plot.png)\n",
    "\n",
    "\n",
    "**Test accuracy before**: 77.0%\n",
    "\n",
    "**Test accuracy after**: 81.1%\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### Task 3e)\n",
    "We had already reached an accuracy of over 80% from the beginning of task 3. Therefor the plots below is the same as from task 3d)\n",
    "The final test accuracy is 81.1%.\n",
    "\n",
    "\n",
    "### Task 3f)\n",
    "\n",
    "\n",
    "In the last improvement we reached an accuracy of 81.1%, and from looking and the validation accuracy it seems like we were getting close to overfitting. The graph flattened out and the early stopping kicked in, which hindered overfitting.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4a)\n",
    "\n",
    "FILL IN ANSWER. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4b)\n",
    "FILL IN ANSWER\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4c)\n",
    "FILL IN ANSWER"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.1 64-bit ('py38': conda)",
   "language": "python",
   "name": "python38164bitpy38condac1f68ca5407a4349b0d7e37676f2fbb3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
