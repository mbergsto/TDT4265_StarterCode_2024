{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a15bb5a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "sys.path.append(os.path.dirname(os.getcwd())) # Include ../SSD in path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16a16d28",
   "metadata": {},
   "source": [
    "# Introduction to SSD\n",
    "\n",
    "This code is much more complex than your prior assignment, and we recommend you to spend some time getting familiar with the code structure.\n",
    "The \"complex\" code structure is made to simplify aspects of deep learning experimentation, and help you structure the usual \"sphagetti\" deep learning code.\n",
    "\n",
    "\n",
    "All scripts in this code requires a configuration file. To **start training**, you can type:\n",
    "```\n",
    "python train.py configs/ssd300.py\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b01ea8e3",
   "metadata": {},
   "source": [
    "## Configuration files\n",
    "The key difference from previous starter codes is the use of configuration files. This enables us to change small parts of the experiment without changing hard-coded values (e.g. learning rate in previous assignments).\n",
    "\n",
    "If you take a look in [`configs/ssd300.py`](../configs/ssd300.py) you will notice a large set of objects describing model architecture (`backbone` and `model`), the optimizer, dataset, data loading, and hyperparameters.\n",
    "\n",
    "To load the config we can write the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "66214f45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving SSD outputs to: outputs/\n"
     ]
    }
   ],
   "source": [
    "from ssd.utils import load_config\n",
    "cfg = load_config(\"../configs/ssd300.py\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "879a680c",
   "metadata": {},
   "source": [
    "`cfg` supports access syntax, where all objects in `configs/ssd300.py` are accessible via their attribute name.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8ddc89b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'feature_extractor': '${backbone}', 'anchors': '${anchors}', 'loss_objective': '${loss_objective}', 'num_classes': 11, '_target_': <class 'ssd.modeling.ssd.SSD300'>}\n"
     ]
    }
   ],
   "source": [
    "print(cfg.model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f7b97d7",
   "metadata": {},
   "source": [
    "If we print `cfg.model`, notice that it returns a dictionary and not the model object itself (which is `SSD300` from [`ssd/modeling/ssd.py`](../ssd/modeling/ssd.py)). This is because the model is defined \"lazily\" (wrapped with a `LazyCall`).\n",
    "\n",
    "To create the model, we have to instantiate it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e8daf7c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SSD300(\n",
      "  (feature_extractor): BasicModel(\n",
      "    (features): Sequential(\n",
      "      (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (1): ReLU(inplace=True)\n",
      "      (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "      (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (4): ReLU(inplace=True)\n",
      "      (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "      (6): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (7): ReLU(inplace=True)\n",
      "      (8): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "      (9): ReLU(inplace=True)\n",
      "      (10): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (11): ReLU(inplace=True)\n",
      "      (12): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "      (13): ReLU(inplace=True)\n",
      "      (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (15): ReLU(inplace=True)\n",
      "      (16): Conv2d(256, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "      (17): ReLU(inplace=True)\n",
      "      (18): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (19): ReLU(inplace=True)\n",
      "      (20): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "      (21): ReLU(inplace=True)\n",
      "      (22): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (23): ReLU(inplace=True)\n",
      "      (24): Conv2d(128, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "      (25): ReLU(inplace=True)\n",
      "      (26): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (27): ReLU(inplace=True)\n",
      "      (28): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1))\n",
      "      (29): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (loss_func): SSDMultiboxLoss(\n",
      "    (sl1_loss): SmoothL1Loss()\n",
      "  )\n",
      "  (regression_heads): ModuleList(\n",
      "    (0): Conv2d(128, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): Conv2d(256, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (2-3): 2 x Conv2d(128, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (4-5): 2 x Conv2d(64, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  )\n",
      "  (classification_heads): ModuleList(\n",
      "    (0): Conv2d(128, 44, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): Conv2d(256, 66, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (2-3): 2 x Conv2d(128, 66, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (4-5): 2 x Conv2d(64, 44, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "from tops.config import instantiate\n",
    "model = instantiate(cfg.model)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f67edd3a",
   "metadata": {},
   "source": [
    "Another example, we can load the first batch of the dataset and run a forward pass with the model:\n",
    "\n",
    "(Task4a needs to be implemented in order for this to run)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "af819ae7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.utils.data.dataloader.DataLoader'>\n",
      "image has the shape: torch.Size([32, 3, 300, 300])\n",
      "boxes has the shape: torch.Size([32, 8732, 4])\n",
      "labels has the shape: torch.Size([32, 8732])\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "Expected that the length of the outputted features to be: 6, but it was: 0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 8\u001b[0m\n\u001b[1;32m      6\u001b[0m gpu_transform \u001b[38;5;241m=\u001b[39m instantiate(cfg\u001b[38;5;241m.\u001b[39mdata_train\u001b[38;5;241m.\u001b[39mgpu_transform)\n\u001b[1;32m      7\u001b[0m batch \u001b[38;5;241m=\u001b[39m gpu_transform(batch)\n\u001b[0;32m----> 8\u001b[0m bbox_delta, confidences \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mimage\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe model predicted anchors with  bbox delta: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbbox_delta\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m and confidences: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfidences\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/.conda/envs/tdt4265/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/tdt4265/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/TDT4265_StarterCode_2024/assignment4/SSD/ssd/modeling/ssd.py:61\u001b[0m, in \u001b[0;36mSSD300.forward\u001b[0;34m(self, img, **kwargs)\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining:\n\u001b[1;32m     60\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mforward_test(img, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m---> 61\u001b[0m features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfeature_extractor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     62\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mregress_boxes(features)\n",
      "File \u001b[0;32m~/.conda/envs/tdt4265/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/tdt4265/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/TDT4265_StarterCode_2024/assignment4/SSD/ssd/modeling/backbones/basic.py:80\u001b[0m, in \u001b[0;36mBasicModel.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     77\u001b[0m     expected_shape \u001b[38;5;241m=\u001b[39m (out_channel, h, w)\n\u001b[1;32m     78\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m feature\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m:] \u001b[38;5;241m==\u001b[39m expected_shape, \\\n\u001b[1;32m     79\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected shape: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mexpected_shape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, got: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfeature\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m:]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m at output IDX: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00midx\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 80\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(out_features) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_feature_shape),\\\n\u001b[1;32m     81\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected that the length of the outputted features to be: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_feature_shape)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, but it was: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(out_features)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     82\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mtuple\u001b[39m(out_features)\n",
      "\u001b[0;31mAssertionError\u001b[0m: Expected that the length of the outputted features to be: 6, but it was: 0"
     ]
    }
   ],
   "source": [
    "dataloader_train = instantiate(cfg.data_train.dataloader)\n",
    "print(type(dataloader_train))\n",
    "batch = next(iter(dataloader_train))\n",
    "for key, item in batch.items():\n",
    "    print(key, \"has the shape:\", item.shape)\n",
    "gpu_transform = instantiate(cfg.data_train.gpu_transform)\n",
    "batch = gpu_transform(batch)\n",
    "bbox_delta, confidences = model(batch[\"image\"])\n",
    "print(f\"The model predicted anchors with  bbox delta: {bbox_delta.shape} and confidences: {confidences.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0f15dce",
   "metadata": {},
   "source": [
    "You might ask yourself, why? At first sight, this seems very complicated rather than plain-old  hard coded values.\n",
    "\n",
    "The reason is easy manipulation of experiments. If I want to run the same experiment, but with a different batch size, I can change it with the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "02b474c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original batch size: 32\n",
      "New batch size: 2\n"
     ]
    }
   ],
   "source": [
    "# Lets print the batch size of the original data loader:\n",
    "print(\"Original batch size:\", dataloader_train.batch_size)\n",
    "cfg.train.batch_size = 2 # Setting the batch size to 2\n",
    "dataloader_train = instantiate(cfg.data_train.dataloader)\n",
    "print(\"New batch size:\", dataloader_train.batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c120d29c",
   "metadata": {},
   "source": [
    "Another reason is **configuration inheritance**.  This allows us to change cirtain aspects of the config without defining the config from scratch.\n",
    "\n",
    "Take a look in [`configs/change_lr.py`](../configs/change_lr.py) and notice that we inherit from the original config file.\n",
    "The only changes done are to the lr."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "655e002d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving SSD outputs to: outputs/\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'train_cpu_transform'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m cfg \u001b[38;5;241m=\u001b[39m \u001b[43mload_config\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m../configs/change_lr.py\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m model \u001b[38;5;241m=\u001b[39m instantiate(cfg\u001b[38;5;241m.\u001b[39mmodel)\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(model)\n",
      "File \u001b[0;32m~/TDT4265_StarterCode_2024/assignment4/SSD/ssd/utils/utils.py:50\u001b[0m, in \u001b[0;36mload_config\u001b[0;34m(config_path)\u001b[0m\n\u001b[1;32m     48\u001b[0m config_path \u001b[38;5;241m=\u001b[39m Path(config_path)\n\u001b[1;32m     49\u001b[0m run_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(config_path\u001b[38;5;241m.\u001b[39mparts[\u001b[38;5;241m1\u001b[39m:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]) \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m config_path\u001b[38;5;241m.\u001b[39mstem\n\u001b[0;32m---> 50\u001b[0m cfg \u001b[38;5;241m=\u001b[39m \u001b[43mLazyConfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mconfig_path\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     51\u001b[0m cfg\u001b[38;5;241m.\u001b[39moutput_dir \u001b[38;5;241m=\u001b[39m Path(cfg\u001b[38;5;241m.\u001b[39mtrain\u001b[38;5;241m.\u001b[39m_output_dir)\u001b[38;5;241m.\u001b[39mjoinpath(\u001b[38;5;241m*\u001b[39mconfig_path\u001b[38;5;241m.\u001b[39mparts[\u001b[38;5;241m1\u001b[39m:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m], config_path\u001b[38;5;241m.\u001b[39mstem)\n\u001b[1;32m     52\u001b[0m cfg\u001b[38;5;241m.\u001b[39mrun_name \u001b[38;5;241m=\u001b[39m run_name\n",
      "File \u001b[0;32m~/TDT4265_StarterCode_2024/assignment4/SSD/tops/config/lazy.py:211\u001b[0m, in \u001b[0;36mLazyConfig.load\u001b[0;34m(filename, keys)\u001b[0m\n\u001b[1;32m    207\u001b[0m             content \u001b[38;5;241m=\u001b[39m f\u001b[38;5;241m.\u001b[39mread()\n\u001b[1;32m    208\u001b[0m         \u001b[38;5;66;03m# Compile first with filename to:\u001b[39;00m\n\u001b[1;32m    209\u001b[0m         \u001b[38;5;66;03m# 1. make filename appears in stacktrace\u001b[39;00m\n\u001b[1;32m    210\u001b[0m         \u001b[38;5;66;03m# 2. make load_rel able to find its parent's (possibly remote) location\u001b[39;00m\n\u001b[0;32m--> 211\u001b[0m         \u001b[43mexec\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mcompile\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcontent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mexec\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodule_namespace\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    212\u001b[0m     ret \u001b[38;5;241m=\u001b[39m module_namespace\n\u001b[1;32m    213\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/TDT4265_StarterCode_2024/assignment4/SSD/configs/change_lr.py:3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtops\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconfig\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LazyCall \u001b[38;5;28;01mas\u001b[39;00m L\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# The line belows inherits the configuration set for the SSD300 mnist config\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mssd300\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m      4\u001b[0m     train,\n\u001b[1;32m      5\u001b[0m     optimizer,\n\u001b[1;32m      6\u001b[0m     schedulers,\n\u001b[1;32m      7\u001b[0m     loss_objective,\n\u001b[1;32m      8\u001b[0m     model,\n\u001b[1;32m      9\u001b[0m     backbone,\n\u001b[1;32m     10\u001b[0m     data_train,\n\u001b[1;32m     11\u001b[0m     data_val,\n\u001b[1;32m     12\u001b[0m     train_cpu_transform,\n\u001b[1;32m     13\u001b[0m     val_cpu_transform,\n\u001b[1;32m     14\u001b[0m     gpu_transform,\n\u001b[1;32m     15\u001b[0m     label_map\n\u001b[1;32m     16\u001b[0m )\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# We can keep all other configs the same, and only change the learning rate to a given value.\u001b[39;00m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# You can now start training with the following command: python train.py configs/change_lr.py\u001b[39;00m\n\u001b[1;32m     19\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mlr \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[0;32m~/TDT4265_StarterCode_2024/assignment4/SSD/tops/config/lazy.py:151\u001b[0m, in \u001b[0;36m_patch_import.<locals>.new_import\u001b[0;34m(name, globals, locals, fromlist, level)\u001b[0m\n\u001b[1;32m    149\u001b[0m exec(\u001b[38;5;28mcompile\u001b[39m(content, cur_file, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexec\u001b[39m\u001b[38;5;124m\"\u001b[39m), module\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__dict__\u001b[39m)\n\u001b[1;32m    150\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m fromlist:  \u001b[38;5;66;03m# turn imported dict into DictConfig automatically\u001b[39;00m\n\u001b[0;32m--> 151\u001b[0m     val \u001b[38;5;241m=\u001b[39m _cast_to_config(\u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;18;43m__dict__\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mname\u001b[49m\u001b[43m]\u001b[49m)\n\u001b[1;32m    152\u001b[0m     module\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__dict__\u001b[39m[name] \u001b[38;5;241m=\u001b[39m val\n\u001b[1;32m    153\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m module\n",
      "\u001b[0;31mKeyError\u001b[0m: 'train_cpu_transform'"
     ]
    }
   ],
   "source": [
    "cfg = load_config(\"../configs/change_lr.py\")\n",
    "model = instantiate(cfg.model)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fcf5864",
   "metadata": {},
   "source": [
    "# Useful commands:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3397a7a7",
   "metadata": {},
   "source": [
    "#### Training and evaluation\n",
    "To start training:\n",
    "```\n",
    "python train.py  configs/ssd300.py\n",
    "```\n",
    "\n",
    "To only run evaluation:\n",
    "```\n",
    "python train.py  configs/ssd300.py --evaluate-only\n",
    "```\n",
    "\n",
    "#### Demo.py\n",
    "\n",
    "For MNIST:\n",
    "```\n",
    "python demo.py configs/ssd300.py demo/mnist demo/mnist_output\n",
    "```\n",
    "\n",
    "\n",
    "#### Runtime analysis:\n",
    "```\n",
    "python3 runtime_analysis.py configs/ssd300.py\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
