{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 1 Report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is an outline for your report to ease the amount of work required to create your report. Jupyter notebook supports markdown, and I recommend you to check out this [cheat sheet](https://github.com/adam-p/markdown-here/wiki/Markdown-Cheatsheet). If you are not familiar with markdown.\n",
    "\n",
    "Before delivery, **remember to convert this file to PDF**. You can do it in two ways:\n",
    "1. Print the webpage (ctrl+P or cmd+P)\n",
    "2. Export with latex. This is somewhat more difficult, but you'll get somehwat of a \"prettier\" PDF. Go to File -> Download as -> PDF via LaTeX. You might have to install nbconvert and pandoc through conda; `conda install nbconvert pandoc`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## task 1a)\n",
    "\n",
    "Intersection over Union (IoU) is a measure used to quantify the accuracy of an object detector on a particular dataset. It calculates the overlap between the predicted bounding box and the ground truth bounding box. IoU is defined as the area of overlap between the two bounding boxes divided by the area of union of the two bounding boxes.\n",
    "\n",
    "Mathematically, it can be expressed as:\n",
    "$$\n",
    "IoU = \\frac{area(B_p \\cap B_{gt})}{area(B_p \\cup B_{gt})}\n",
    "$$\n",
    "\n",
    "Where $B_p$ is the predicted bounding box and $B_{gt}$ is the ground truth bounding box. \n",
    "\n",
    "### Drawing:\n",
    "\n",
    "<img src=\"images/task1a.jpeg\" width=\"1000\" height=\"600\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## task 1b)\n",
    "\n",
    "$\\textbf{Precision}$ measures the accuracy of the positive predictions, i.e., the proportion of correctly identified positive cases out of all cases that the model predicted as positive.\n",
    "$$\n",
    "Precision = \\frac{True\\:Positives}{True\\:Positives+False\\:Positives}\n",
    "$$\n",
    "\\\n",
    "$\\textbf{Recall}$ measures the ability of the model to find all the relevant cases within a dataset, i.e., the proportion of correctly identified positive cases out of all actual positive cases.\n",
    "$$\n",
    "Recall = \\frac{True\\:Positives}{True\\:Positives+False\\:Negatives}\n",
    "$$\n",
    "\n",
    "- $\\textbf{True Positives (TP)}$ are the correctly identified positive cases.\n",
    "- $\\textbf{False Positives (FP)}$ are the negative cases incorrectly classified as positive.\n",
    "\n",
    "## task 1c)\n",
    "\n",
    "\n",
    "<img src=\"images/task1c-1.jpeg\" width=\"1000\" height=\"600\">\n",
    "<img src=\"images/task1c-2.jpeg\" width=\"1000\" height=\"900\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2\n",
    "\n",
    "### Task 2f)\n",
    "\n",
    "<img src=\"task2/precision_recall_curve.png\" width=\"1000\" height=\"900\">\n",
    "\n",
    "#### Mean average precision: 0.9066\n",
    "\n",
    "The precision-recall curve shows us how well the model returns relevant results vs. how well the model detects relevant instances.\\\n",
    "In other words: how well the model can identify true positives and avoid false positives. \n",
    "\n",
    "Here are some observations from the curve:\n",
    "\n",
    "- High Precision at High Recall: The curve starts at the top left corner, indicating a high precision close to 1.0 even at lower recall levels. This suggests that when the model claims an object is present, it is usually correct, which is especially the case when the recall is between 0.8 and 0.9.\n",
    "\n",
    "- Stable Performance: The curve stays relatively flat as recall increases, indicating that precision is maintained as the model identifies more true positives. This is a sign of good performance, as the model does not sacrifice much precision while increasing recall until a certain point.\n",
    "\n",
    "- Sharp Drop in Precision: At recall levels above approximately 0.925, precision drops sharply to below 0.85. This indicates that the model starts to make more false positive errors as it tries to capture almost all true positives."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3a)\n",
    "Fill in task 1a image of hand-written notes which are easy to read, or latex equations here\n",
    "\n",
    "### Task 3b)\n",
    "Fill in task 1a image of hand-written notes which are easy to read, or latex equations here\n",
    "\n",
    "### Task 3c)\n",
    "Fill in task 1a image of hand-written notes which are easy to read, or latex equations here\n",
    "\n",
    "\n",
    "### Task 3d)\n",
    "Fill in task 1a image of hand-written notes which are easy to read, or latex equations here\n",
    "\n",
    "### Task 3e)\n",
    "Fill in task 1a image of hand-written notes which are easy to read, or latex equations here\n",
    "\n",
    "### Task 3f)\n",
    "Fill in task 1a image of hand-written notes which are easy to read, or latex equations here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4b)\n",
    "\n",
    "FILL IN ANSWER. \n",
    "\n",
    "## Task 4c)\n",
    "FILL IN ANSWER. \n",
    "\n",
    "\n",
    "## Task 4d)\n",
    "FILL IN ANSWER. \n",
    "\n",
    "\n",
    "## Task 4e)\n",
    "FILL IN ANSWER. \n",
    "\n",
    "\n",
    "## Task 4f)\n",
    "FILL IN ANSWER. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.1 64-bit ('py38': conda)",
   "language": "python",
   "name": "python38164bitpy38condac1f68ca5407a4349b0d7e37676f2fbb3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
